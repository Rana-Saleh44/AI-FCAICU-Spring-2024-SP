Chapter Outline *check the vid slide 3,7
◼ Agents and Environments
◼ How Agents Should Act
◼ Rational Agents                                        
◼ Task Environment of Agent
◼ Structure of Agents

rational agent(program)-> making decisions aka reasoning->goal ->based on available info and understanding the environment.

perceive be aware of something through sense" sight, hearing ,etc"

The Big AI Picture -> agent (exist in an environment and perceive, act, and learn)
1: Sense from the environment.
2: Reason inside it!! see slide 2 arrow.
3: Act in the environment.
4: Get Feedback form the environment.
5: Learn "agent".

An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.
Human agent has:
– Eyes, ears, and other organs for sensors;
– Hands, legs, mouth, and other body parts for actuators.
◼ Robotic agent has:
– Cameras and infrared range finders for sensors;
– Various motors for actuators.
◼ Software agent:
– Receives keystrokes, file contents, and network packets as inputs;
– Displays on screen, writes files, and sends net packets as outputs.

Agents interact with environments through sensors and actuators.

◼ Mathematically, an agent’s behavior is described by the agent function that maps from percept histories to actions:
                   [ f: P* → A ]

agent function -> implemented by -> agent program -> running on -> agent architecture -> to produce -> f
                   agent = architecture + program

example:
Vacuum-cleaner world
◼ Environment: just two locations – squares A and B
◼ Percepts: which square it is in (location) and whether there is dirt in the square (contents), e.g., [A, Dirty]
◼ Actions: move left, move right, suck up the dirt, or do nothing
◼ Agent function: if the current square is dirty, then suck; otherwise move to the other square.

How Agents Should Act
◼ A rational agent is one that does “the right thing” based on what it can perceive and the actions it can perform.
◼ The right action is the one that will cause the agent to be most successful
–  how and when to evaluate the agent's success.
◼performance measure for the how

How Agents Should Act
not one fixed measure suitable for all agents.
– There are different criteria for different agents.
◼ Outside observers establish a standard of what it means to be successful in an environment and use it to measure the performance of agents.
– E.g., performance measure of a vacuum-cleaner agent could be amount of dirt cleaned up, amount of time taken, amount of electricity consumed, amount of noise generated, etc.

example slide 10

Rationality vs. Success
◼ Note that: rational ≠ successful
◼ Rationality depends not only on the performance measure, but also on:
– Agent’s sensory capabilities (what it can sense).
» Coffee-agents that detect obstacles visually vs. those that use collision detection.
– Agent’s actuator capabilities (what it can do).
» Coffee-agents that can heat coffee with a built-in coil vs. those that cannot.
– Agent’s knowledge.
» Informed agents, uninformed agents, misinformed agents.

Rational Agents
◼ What is rational at any given time depends on four
things:
– The performance measure that defines the criterion of success.
– The agent’s prior knowledge of the environment.
– The actions that the agent can perform.
– The agent’s percept sequence to date.
◼ Rational Agent Definition:
For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.

Rational Agents
◼ Need to distinguish between rationality and omniscience
– An omniscient agent (all-knowing with infinite knowledge) knows the actual outcome of its actions, and can act accordingly.
– But omniscience is impossible in reality.
◼ Rationality is not the same as perfection
– Rationality maximizes expected performance, while perfection maximizes actual performance.
◼ Agents, to be rational, can perform actions to obtain useful information in order to modify future percepts (information gathering, exploration).
◼ A rational agent should be autonomous
– Its behavior is determined by its own experience (with ability to learn and adapt its prior knowledge).

Specifying the Task Environment
◼ To design an agent, the first step is to specify its task environment as fully as possible.
◼ The task environment of any agent includes:
1. Performance measure.
2. Environment.
3. Actuators.
4. Sensors.
◼ For the acronymically minded, we call this the PEAS (Performance, Environment, Actuators, Sensors) description.

slide 15,16,17,18 example

Properties of Task Environments
◼ The task environments can be categorized according to number of dimensions:
– Fully observable (vs. partially observable): An agent's sensors give it access to the complete state of the environment at each point in time.
– Single agent (vs. multi-agent): An agent operating by itself in an environment.
– Deterministic (vs. stochastic): Next state of the environment is completely {determined} by the current state and the action executed by the agent.
– Episodic (vs. sequential): In an episodic task environment, the agent's experience is divided into atomic episodes – each episode consists of the agent percept and then performing a single action.
– Dynamic vs. static: If the environment can change while an agent is deliberating.
– Discrete (vs. continuous): A limited number of distinct,
clearly defined percepts and actions.
– Known (vs. unknown): The rules of the game, or physics/dynamics of the environment are known to the agent.
◼ These properties of various task environments determine the appropriate agent design and the applicable technique for agent implementation.

– Fully observable (vs. partially observable):
» A task environment is effectively fully observable if the sensors detect all aspects that are relevant (depends on the performance measure) to the choice of action.
» Fully observable environments are convenient because the agent need not maintain any internal state to keep track of the world.
» An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data:
» If the agent has no sensors at all then the environment is unobservable.

– Single agent (vs. multi-agent): An agent operating by itself in an environment.
» For example, an agent solving a crossword puzzle by itself is clearly in a single-agent environment, whereas an agent playing chess is in a two-agent environment.
» Chess is a competitive multi-agent environment.
◼ In chess, the opponent entity B is trying to maximize its performance measure, which, by the rules of chess, minimizes agent A’s performance measure.
» In the taxi-driving environment, on the other hand, avoiding collisions maximizes the performance measure of all agents, so it is a partially cooperative multi-agent environment.

– Episodic (vs. sequential): 
» Choice of action in each episode depends only on the episode itself.
◼ Next episode does not depend on the actions taken in previous episodes.
» In sequential environments, the current decision could affect all future decisions.
◼ Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences.
» Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.

– Deterministic (vs. stochastic):
» In principle, an agent need not worry about uncertainty in a fully observable, deterministic environment.
◼ If the environment is deterministic except for the actions of other agents; in a multi-agent environment, then the environment is strategic.
◼ Thus, a game can be deterministic even though each agent may be unable to predict the actions of the others.
» If the environment is partially observable, however, then it could appear to be stochastic.
◼ Most real situations are so complex that it is impossible to keep track of all the unobserved aspects; therefore, they must be treated as stochastic.
» It is often better to think of an environment as deterministic or stochastic from the point of view of the agent.

– Dynamic vs. static:
» Static environments are easy to deal with.
◼ Because the agent need not keep looking at the world while it is deciding on an action, nor need it worry about the passage of time.
» Dynamic environments, on the other hand, are continuously asking the agent what it wants to do.
◼ If it hasn’t decided yet, that counts as deciding to do nothing.
» If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is semidynamic.

– Discrete (vs. continuous):
» This distinction applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent.
» For example:
◼ Chess environment has a finite number of distinct states (excluding the clock) and a discrete set of percepts and actions.
◼ Taxi driving is a continuous-state and continuous-time problem: the speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time, and Taxi-driving actions are also continuous (steering angles, etc.).
◼ Input from digital cameras is discrete, strictly speaking, but is typically treated as representing continuously varying intensities and locations.

– Known (vs. unknown):
» In a known environment, the outcomes (or outcome probabilities if the environment is stochastic) for all actions are given.
» Obviously, if the environment is unknown, the agent will have to learn how it works in order to make good decisions.
» The distinction between known and unknown environments is not the same as the one between fully and partially observable environments.
◼ It is possible for a known environment to be partially observable—for example, in solitaire card games, I know the rules but am still unable to see the cards that have not yet been turned over.
◼ Conversely, an unknown environment can be fully observable—in a new video game, the screen may show the entire game state but I still don’t know what the buttons do until I try them.

Properties of Task Environments  check slide 28 
◼ The hardest case is partially observable, multi-agent, stochastic, sequential, dynamic, continuous, and unknown.

Structure of Agents
AI job in the program not the architecture and the architecture affect the program.
◼ Obviously, the designed program for an agent must to be appropriate for that agent architecture.
                Agent = Architecture + Program
◼ The agent architecture->computing device, the program run on it, with physicalsensors and actuators.
◼ The architecture has three functions:
1. Makes the percepts from sensors available to the program.
2. Runs the program.
3. Feeds the generated program’s action to the actuators.

Skeleton Agent check slide 30!!
Table-Driven Agent slide 33!!

Skeleton Agent "general structure of the agent"
◼ On each invocation:
– the agent's memory is updated to reflect the new percept
– the best action is chosen
– the action was taken is also stored in memory
◼ The memory persists from one invocation to the next.
◼ It is up to the agent, if it so desires, to build up the percept sequence in memory.
– In some environments, it is possible to be quite successful without storing the percept sequence.
– In complex domains, it is infeasible to store the complete sequence.
◼ The performance measure; not part of skeleton program; is applied {externally} to judge the behavior of the agent. momkn ykoun fe tkyem da5lly lnfsh 34an y7sn mn performance.

Table-Driven Agent {1st idea}
◼ Operates by keeping in memory its percept sequence->index into table, which contains the appropriate action for all possible percept sequences.
◼ Why this proposal is doomed to failure?
1. The table needed for something as {simple, little num of percepts and actions}->agent that can only play chess{35^100} would be about 35100 entries.
2. The designer will take quite a long time to build the table.
3. The agent has no autonomy at all, if the environment changed in some unexpected way, the agent would be lost.
4. Even if the agent has a learning mechanism as well, to have a degree of autonomy, it would take forever to learn the right table entries.
◼ AI challenge is to find out how to write a program that produce rational behavior from a small amount of code rather than from a large number of table entries.
even I didn't make it know how to build a table !! dr

Agent Program
◼ We will consider four types of agent program that represent the principles underlying of almost all intelligence systems:
– Simple reflex agents->fully obserable
– Model-based reflex agents-> partially observable-> it must have a model
both the previous there are uncertainty and only the reflex there are no goal that it want to achieve.
– Goal-based agents-> goal
– Utility-based agents-> technique mda 2rbouh aw b3douh mn el goal. "heurestic search" and also I could have multi-goal{better goal and better action}

reflex->automatic reaction..like pulling your hand away from something hot without thinking about it.
current percepts->multiple sensors

Simple Reflex Agents check slide 35,37!!
Simple Reflex Agents
◼ Simplest kind of agents.
◼ These agents select actions on the basis of the {current percept}, {ignoring}->percept history. therefore it could not have memory
◼ condition-action rules->percept to action.
◼ The agent program is very simple:
– The INTERPRET-INPUT function generates an abstracted description of the current state from the percept.
– The RULE-MATCH function returns the first rule in the set of rules that matches the given state description.
◼ Work {only} if the environment is {fully observable}.

Model-based Reflex Agents check slide 39,40
◼ {Most effective} way to handle {partial observability} is to keep track of part of the world it can not see now.
– The agent {maintains}some sort of {internal state}that depends on {percept history} and reflects some aspects of the current state.
** after making the action and get the feedback it modify the internal state.
◼ Two kinds of knowledge->model->must be encoded in the agent program->to update the internal state information:
– First,some info about how the {world evolves independently} of the agent.
– Second,some info about how the {agent's own actions} affect the world.
◼ The agent structure showing how the current percept is combined with the old internal state to generate the updated description of the current state.
◼ Details of how models and states are represented vary widely depending on the type of environment and the particular technology used in the agent design.
◼ Regardless of the kind of representation used, it is seldom"rare" possible for the agent to determine exactly the current state of a partially observable environment.
– For that reason, uncertainty about the current state may be unavoidable, but the agent still has to make a decision.
– Therefore, the internal “state” maintained by a model-based agent may be not describe “what the world is like now” in a literal sense. 
environment evolve POINT!!????

Goal-based Agents check slide 43,44   {the additional of the previous agent is that +goal to the model  & the decesion is different}
many rule match I'll choose that'll achieve the goal.
◼ Knowing about the {current state} of the environment is {not always enough} to decide what to do.
– The agent needs some sort of goal information, which describes situations that are desirable.
◼ The agent program can combine the "goal with the model" (same information used in the model-based reflex agent) in order to choose actions that achieve that goal.
– {Sometimes}, goal-based action will be simple, when goal satisfaction results immediately from a single action.
– {Sometimes}, it will be more tricky, when the agent has to consider many trials to find a way to achieve the goal.
◼ Search and planning {if the environment is determintistic,static}are the subfields of AI devoted to finding action sequences that achieve the agent’s goals.
◼{{ Decision making of this kind is different from the condition–action rules described earlier.}}
– It involves consideration of the future for both:
» What will happen if I do such-and-such?
» Will that make me happy?
◼ This agent is {more flexible} because the knowledge that supports its decisions is{ represented explicitly and can be modified}.

Utility-based Agents  check slide 47,48
◼ {Goals alone} are {not really enough} to generate high-quality behavior in most environment.
– Goals just provide a crude binary distinction between “happy” and “unhappy” states.  {without considering any nuances or details in between.}
– Need a comparison of different world states according to exactly how happy they would make the agent.
◼ Therefore, the agent wants some sort of {utility function} that {internally measures its performance} among different states of the world.
–> If the internal utility function and the external performance measure are in agreement
– Then an agent that chooses actions to maximize its utility will be rational according to the external performance measure
◼ In two kinds of cases, goals are inadequate{insufficient or not enough} but a utility-based agent can still make rational decisions:
– First, when there are {conflicting goals}, only some of which can be achieved,
» The utility function specifies the appropriate trade-off.
– Second, when there are {several goals} that the agent can aim for, none of which can be achieved with certainty,
» Utility provides a way in which the likelihood of {success can be weighed up against the importance of the goals.}
◼ Technically, a rational utility-based agent chooses the action that maximizes the {expected utility} of the action outcomes.
it is also heuristic there are some sort of unceertainty
high experience high performane 

Learning Agents check slide 50 important chagpt
performance elem.->agent
+
feedback of the action->internal evaluting da 8er external observer

Summary
◼ Agents interact with environments through actuators and sensors.
◼ Agent function specifies the action taken by the agent in response to any percept sequence.
◼ Performance measure evaluates the environment sequence.
◼ A rational agent maximizes expected performance.
◼ Agent programs implement (some) agent functions.
◼ PEAS descriptions define task environments.
◼ Environments are categorized along several dimensions:
– observable? deterministic? episodic? static? discrete? single-agent?
◼ Several basic agent architectures exist:
– simple reflex, model-based, goal-based, utility-based
◼ All agents can improve their performance through learning.

deterministic conditional strategic
